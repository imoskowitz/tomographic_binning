{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a666e2-97b4-44d9-a98b-75d0d6f08032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classifier funbins\n",
      "Found classifier IBandOnly\n",
      "Found classifier RandomForest\n",
      "Found classifier Random\n"
     ]
    }
   ],
   "source": [
    "import tomo_challenge as tc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52273d2-7826-4f01-bbdd-23e14886a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_buzzard.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/u2/i/irenem/tomo_challenge/tomo_challenge/data.py:85: UserWarning: Setting inf (undetected) bands to mag=30\n",
      "  warnings.warn(\"Setting inf (undetected) bands to mag=30\")\n"
     ]
    }
   ],
   "source": [
    "import tomo_challenge as tc\n",
    "#format input data from tomo challenge\n",
    "bands = 'ugrizy'\n",
    "is_buzzard = True\n",
    "if is_buzzard:\n",
    "    training_file = 'training_buzzard.hdf5'\n",
    "    testing_file = 'validation_buzzard.hdf5'\n",
    "    train_frac = 0.13\n",
    "    test_frac = 0.37\n",
    "    keep_frac = 0.340\n",
    "    sample_name = 'buzzard'\n",
    "if not is_buzzard:\n",
    "    training_file = 'training.hdf5'\n",
    "    testing_file = 'validation.hdf5'\n",
    "    train_frac = 0.084\n",
    "    test_frac = 0.23\n",
    "    keep_frac = 0.213\n",
    "    sample_name = 'DC2'\n",
    "\n",
    "print(training_file)\n",
    "\n",
    "include_colors = True\n",
    "include_errors = True\n",
    "\n",
    "\n",
    "#application data\n",
    "data = tc.load_data(testing_file, bands, errors=include_errors, colors=include_colors, size=True, array=False)\n",
    "data_z = tc.load_redshift(testing_file)\n",
    "data = pd.DataFrame.from_dict(data)\n",
    "\n",
    "#adding redshifts\n",
    "#training_data['specz'] = training_z\n",
    "data['specz'] = data_z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(f'object_ratios_testing_{sample_name}') as json_file:\n",
    "            ratios = json.load(json_file)\n",
    "        \n",
    "ratios_list = []\n",
    "for i in range(len(ratios)):\n",
    "    ratios_list.append(ratios[f'{i}'])\n",
    "data['ratios'] = ratios_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40584b2b-16d6-48fc-81a9-0d90b10885d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file function\n",
    "def generate_input_files(directory, filename = 'tpzrun', inputdf = data, tpz_nnr_nnc_split = np.array([0.333, 0.666]), nn_train_val_split = 0.85, overall_split = 0.8, no_nnr = True, sample_frac = keep_frac, subrun = False):\n",
    "\n",
    "    # directory: The location of the run where you want to save all the files\n",
    "    # filesname: The name of all the files for this run\n",
    "    # inputfp: The file path of the input catalog\n",
    "    # tpz_nnr_nnc_split: The split of the data for TPZ/NNR/NNC training; default is in thirds (or in halves if there is no NNR)\n",
    "    # nn_train_val_split: Fraction of objects assigned to each NN for training that will be used for training (as opposed to validation)\n",
    "    # overall_split: The fraction of the input catalog that will be used for training TPZ/NNR/NNC.  The remaining objects will be the \"application set\"\n",
    "    # train_frac_lim: Fraction of training sample you would like to use.  Should typically behttps://docs.google.com/presentation/d/1bZtAba8dzczmLLvutQDPrBBSjbTU1KS5hxAXAlq2A-I/edit?usp=sharing 1. \n",
    "\n",
    "    # NOTE TO IRENE\n",
    "    # Don't forget, you will need to replace default_test_train_split with your own function for splitting the indices into training and application indices.  It should split them using overall_split\n",
    "    # Write a function called get_features that takes a list of indices as an argument and outputs (specz, photometric_features, photometric_feature_errs) for those objects.  photometric_features (and errs) should have shape NxM where N is the number of objects and M is the number of features\n",
    "    # Write a function to replace hsc_select that returns all the indices of objects that are acceptable for training/fitting.  No NaNs, negative magnitudes, etc.  You may want to apply a signal to noise cut, etc...\n",
    "\n",
    "    # Here is the run order:\n",
    "    # 1) Generate Files (will make indexing files and TPZ train/test/application files)\n",
    "    # 2) Train TPZ (mpiexec -n <NUMCORES> runmlz <filename>.inputs)\n",
    "    # 3) Run TPZ on application set (mpiexec -n <NUMCORES> runMLZ --modify testfile=<filename>.tpz_app --no_train <filename>.inputs)\n",
    "    # 4) Generate Files (will make NNR and NNC train/test/application files)\n",
    "    # 5) Train NNC (nnc = nnclass.nn_classifier(run_folder = <directory>, data_name = <filename>))\n",
    "    # 6) Run NNC on application set (nnc.fit_app())\n",
    "    # At this point, your important outputs will be in the .nn_app file (for the TPZ features) and the results_application.dat file within the NNC's output folder (which has all the NNC confidence values)\n",
    "\n",
    "\n",
    "    #If there isn't a NNR running, just set the default split to 50/50\n",
    "\n",
    "    if no_nnr and all(tpz_nnr_nnc_split == np.array([0.333, 0.666])):\n",
    "        tpz_nnr_nnc_split = np.array([0.5, 0.5])\n",
    "\n",
    "    # =======================\n",
    "    # GENERATE INDEXING FILES\n",
    "    # =======================\n",
    "\n",
    "    if not os.path.isfile(directory + filename + '.train_inds') and not os.path.isfile(directory + filename + '.test_inds'):\n",
    "\n",
    "        print('Generating fresh indexing files in %s' % directory)\n",
    "\n",
    "        # indices should contain the integer indices of objects in the catalog that are good for training/testing/etc.\n",
    "        #If using every object, set cuts=False\n",
    "        all_indices = data_select_testing(inputdf, unrep=False, sample=sample_name)\n",
    "\n",
    "        if sample_frac !=1:\n",
    "            np.random.shuffle(all_indices)\n",
    "            selected_indices = np.sort(all_indices[:int(len(all_indices)*sample_frac)])\n",
    "         \n",
    "        print(len(selected_indices))\n",
    "        selected_data = inputdf.iloc[selected_indices,:]\n",
    "       \n",
    "        training_indices = data_select_testing(selected_data, unrep=True, sample=sample_name)\n",
    "        testing_indices = data_select_training(selected_data, unrep=True, sample=sample_name)\n",
    "\n",
    "        # Here, it has split indices into two lists, which are the OVERALL train and test indices (for the whole pipeline, not a single element; this \"test_inds\" is essentially the application set).\n",
    "        #train_inds, test_inds = default_test_train_split(indices, splitfrac = overall_split)\n",
    "        train_inds = training_indices\n",
    "        test_inds = testing_indices\n",
    "\n",
    "        print('training and testing index lengths')\n",
    "        print(len(train_inds), len(test_inds))\n",
    "\n",
    "        #Shuffle the training indices and assign objects as training objects for TPZ, NNR, and NNC\n",
    "\n",
    "        scrambled_train = np.arange(len(train_inds))\n",
    "        np.random.shuffle(scrambled_train)\n",
    "\n",
    "        tpz_indicator = np.zeros(len(train_inds), dtype = int)\n",
    "        nnr_indicator = np.zeros(len(train_inds), dtype = int)\n",
    "        nnc_indicator = np.zeros(len(train_inds), dtype = int)\n",
    "\n",
    "        tpz_split, nnr_split, nnc_split = np.split(scrambled_train, (tpz_nnr_nnc_split * len(scrambled_train)).astype(int))\n",
    "\n",
    "        tpz_indicator[tpz_split] = 1\n",
    "        nnr_indicator[nnr_split] = 1\n",
    "        nnc_indicator[nnc_split] = 1\n",
    "\n",
    "       ## Save some files that have flags for what trains on what\n",
    "\n",
    "        np.savetxt(directory + filename + '.train_inds', np.vstack((train_inds, tpz_indicator, nnr_indicator, nnc_indicator)).T, header = 'train_inds  tpz_train  nnr_train  nnc_train', fmt = '%i  %i  %i  %i')\n",
    "        np.savetxt(directory + filename + '.app_inds', test_inds, header = 'test_inds', fmt = '%i')\n",
    "\n",
    "        print('finished: generate indexing files')\n",
    "\n",
    "        generate_input_files(directory, filename, inputdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, sample_frac, subrun = True)\n",
    "   # ==================\n",
    "   # GENERATE TPZ FILES\n",
    "   # ==================\n",
    "    elif not os.path.isfile(directory + filename + '.tpz_train') and not os.path.isfile(directory + filename + '.tpz_test'):\n",
    "\n",
    "        print('Generating fresh TPZ input files in %s' % directory)\n",
    "\n",
    "        train_inds, tpz_indicator, nnr_indicator, nnc_indicator = np.loadtxt(directory + filename + '.train_inds', unpack = True, dtype = int)\n",
    "\n",
    "        tpz_indicator = tpz_indicator.astype(bool)\n",
    "        nnr_indicator = nnr_indicator.astype(bool)\n",
    "        nnc_indicator = nnc_indicator.astype(bool)\n",
    "\n",
    "        tpz_train_inds = train_inds[tpz_indicator]\n",
    "        tpz_test_inds = train_inds[nnr_indicator | nnc_indicator]  # The TPZ test indices are all the objects in the pipeline training set that won't be used by the NNC or NNR\n",
    "\n",
    "        # Make a function called 'get_features' that returns the object specz's, all relevant photometric features, and their errors\n",
    "        train_specz, train_features, train_feature_errs = get_features(tpz_train_inds, inputdf, size=False)\n",
    "        test_specz, test_features, test_feature_errs = get_features(tpz_test_inds, inputdf, size=False)\n",
    "\n",
    "        header = 'specz g r i z y gr ri iz zy gri riz izy eg er ei ez ey egr eri eiz ezy egri eriz eizy'\n",
    "        fmt = ' %.10f' + ' %.5f'*5 + ' %.5e' * 7 + ' %.5e'*12\n",
    "\n",
    "        # Save the TPZ training and test set files - after this step, you're going to have to run TPZ before anything else will happen\n",
    "\n",
    "        np.savetxt(directory + filename + '.tpz_train', np.hstack((train_specz.reshape(-1,1), train_features, train_feature_errs)), header = header, fmt = fmt)\n",
    "        np.savetxt(directory + filename + '.tpz_test', np.hstack((test_specz.reshape(-1,1), test_features, test_feature_errs)), header = header, fmt = fmt)\n",
    "        print('finished: generate tpz files')\n",
    "\n",
    "        generate_input_files(directory, filename, inputdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, sample_frac, subrun = True)\n",
    "        #generate_input_files(directory, filename, inputdf, testingdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, train_frac_lim, test_frac_lim, subrun = True)\n",
    "\n",
    "    # ==================\n",
    "    # GENERATE NNR FILES\n",
    "    # ==================\n",
    "\n",
    "    elif (os.path.isfile(directory + 'output/results/' + filename + '.0.mlz') and not \n",
    "            (os.path.isfile(directory + filename + '.nnr_train') or \n",
    "                os.path.isfile(directory + filename + '.nnr_test') or \n",
    "                os.path.isfile(directory + filename + '.nnr_validate'))):\n",
    "\n",
    "        # This section likely isn't needed since we aren't running the NNR, but I don't want to accidentally break the rest and it just generates a few extra files\n",
    "\n",
    "        print('Generating fresh NNR input files in %s' % directory)\n",
    "\n",
    "        train_inds, tpz_indicator, nnr_indicator, nnc_indicator = np.loadtxt(directory + filename + '.train_inds', dtype = int, unpack = True)\n",
    "\n",
    "        tpz_indicator = tpz_indicator.astype(bool)\n",
    "        nnr_indicator = nnr_indicator.astype(bool)\n",
    "        nnc_indicator = nnc_indicator.astype(bool)\n",
    "\n",
    "        #Scramble the training objects for the NNR to produce its training and validation files\n",
    "\n",
    "        train_inds_scrambled = np.copy(train_inds[nnr_indicator])\n",
    "        np.random.shuffle(train_inds_scrambled)\n",
    "\n",
    "        nnr_train_inds, nnr_validate_inds = np.split(train_inds_scrambled, [int(nn_train_val_split*len(train_inds_scrambled))])\n",
    "        nnr_train_inds.sort()\n",
    "        nnr_validate_inds.sort()\n",
    "\n",
    "        np.savetxt(directory + filename + '.nnr_train_inds', nnr_train_inds, fmt = '%i')\n",
    "        np.savetxt(directory + filename + '.nnr_val_inds', nnr_validate_inds, fmt = '%i')\n",
    "\n",
    "        nnr_test_inds = train_inds[nnc_indicator]\n",
    "\n",
    "        # Get the features for the correct indices\n",
    "\n",
    "        nnr_train_specz, nnr_train_features, nnr_train_feature_errs = get_features(nnr_train_inds, inputdf, size=True)\n",
    "        nnr_validate_specz, nnr_validate_features, nnr_validate_feature_errs = get_features(nnr_validate_inds, inputdf, size=True)\n",
    "        nnr_test_specz, nnr_test_features, nnr_test_feature_errs = get_features(nnr_test_inds, inputdf, size=True)\n",
    "\n",
    "        # Load the TPZ outputs because the NNR needs them for training\n",
    "\n",
    "        tpz_features = np.loadtxt(directory + 'output/results/' + filename + '.0.mlz', usecols = [2,4,6]) # 1,3,5 corresponds to zmode; 2,4,6 corresponds to zmean; is z, zconf, zerr\n",
    "\n",
    "        nnr_indicator_trans = nnr_indicator[~tpz_indicator]\n",
    "        nnc_indicator_trans = nnc_indicator[~tpz_indicator]\n",
    "\n",
    "        # Build the features\n",
    "\n",
    "        nnr_train_tpz_features, nnr_validate_tpz_features = np.split(tpz_features[nnr_indicator_trans], [int(nn_train_val_split*np.sum(nnr_indicator_trans))])\n",
    "        nnr_test_tpz_features = tpz_features[nnc_indicator_trans]\n",
    "\n",
    "        header = 'specz size g r i z y gr ri iz zy gri riz izy zphot zconf zerr eg er ei ez ey egr eri eiz ezy egri eriz eizy'\n",
    "        fmt = ' %.10f' + ' %.5f'*6 + ' %.5e' * 7 + ' %.5e'*12 + ' %.5e'*3\n",
    "\n",
    "        # Save all the NNR files\n",
    "\n",
    "        np.savetxt(directory + filename + '.nnr_train', np.hstack((nnr_train_specz.reshape(-1,1), nnr_train_features, nnr_train_tpz_features, nnr_train_feature_errs)), comments = '', header = header, fmt = fmt)\n",
    "        np.savetxt(directory + filename + '.nnr_validate', np.hstack((nnr_validate_specz.reshape(-1,1), nnr_validate_features, nnr_validate_tpz_features, nnr_validate_feature_errs)), comments = '', header = header, fmt = fmt)\n",
    "        np.savetxt(directory + filename + '.nnr_test', np.hstack((nnr_test_specz.reshape(-1,1), nnr_test_features, nnr_test_tpz_features, nnr_test_feature_errs)), comments = '', header = header, fmt = fmt)\n",
    "        print('finished: generate NNR files')\n",
    "    \n",
    "        generate_input_files(directory, filename, inputdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, sample_frac, subrun = True)\n",
    "        #generate_input_files(directory, filename, inputdf, testingdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, train_frac_lim, test_frac_lim, subrun = True)\n",
    "\n",
    "    # ==================\n",
    "    # GENERATE NNC FILES\n",
    "    # ==================\n",
    "\n",
    "    elif (os.path.isfile(directory + 'output/results/' + filename + '.0.mlz') and not \n",
    "            (os.path.isfile(directory + filename + '.nnc_train') or \n",
    "                os.path.isfile(directory + filename + '.nnc_test') or \n",
    "                os.path.isfile(directory + filename + '.nnc_validate'))):\n",
    "\n",
    "        print('Generating fresh NNC input files in %s' % directory)\n",
    "        \n",
    "        train_inds, tpz_indicator, nnr_indicator, nnc_indicator = np.loadtxt(directory + filename + '.train_inds', dtype = int, unpack = True)\n",
    "\n",
    "        tpz_indicator = tpz_indicator.astype(bool)\n",
    "        nnr_indicator = nnr_indicator.astype(bool)\n",
    "        nnc_indicator = nnc_indicator.astype(bool)\n",
    "\n",
    "        # Read in the TPZ features from the NNR test set file\n",
    "\n",
    "        nnr_test_inds = train_inds[nnc_indicator]\n",
    "        nnr_test_specz, nnr_test_features, nnr_test_feature_errs = get_features(nnr_test_inds, inputdf, size=True)\n",
    "        nnr_test_tpz_features = pd.read_csv(directory + filename  + '.nnr_test', delimiter = '\\s+', comment = '#')[['zphot', 'zconf', 'zerr']].to_numpy()\n",
    "\n",
    "        translation_inds = np.arange(len(nnr_test_inds))\n",
    "        np.random.shuffle(translation_inds)\n",
    "\n",
    "        # Split the training set in to training and validation\n",
    "\n",
    "        nnc_train_inds, nnc_validate_inds = np.split(nnr_test_inds[translation_inds], {int(nn_train_val_split*len(nnr_test_inds))})\n",
    "        train_trans, val_trans = np.split(translation_inds, [int(nn_train_val_split*len(nnr_test_inds))])\n",
    "\n",
    "        np.savetxt(directory + filename + '.nnc_train_inds', nnc_train_inds, fmt = '%i')\n",
    "        np.savetxt(directory + filename + '.nnc_val_inds', nnc_validate_inds, fmt = '%i')\n",
    "\n",
    "        np.savetxt(directory + filename + '.nnc_train_inds', np.vstack((nnc_train_inds, train_trans)).T, fmt = '%i  %i', header = 'HSC_ind  NNR_ind')\n",
    "        np.savetxt(directory + filename + '.nnc_validate_inds', np.vstack((nnc_validate_inds, val_trans)).T, fmt = '%i  %i', header = 'HSC_ind  NNR_ind')\n",
    "\n",
    "        # Build the features that will go in the train and validate files\n",
    "\n",
    "        nnc_train_features = nnr_test_features[train_trans]\n",
    "        nnc_validate_features = nnr_test_features[val_trans]\n",
    "        \n",
    "        nnc_train_feature_errs = nnr_test_feature_errs[train_trans]\n",
    "        nnc_validate_feature_errs = nnr_test_feature_errs[val_trans]\n",
    "        \n",
    "        nnc_train_specz = nnr_test_specz[train_trans]\n",
    "        nnc_validate_specz = nnr_test_specz[val_trans]\n",
    "        \n",
    "        nnc_train_tpz_features = nnr_test_tpz_features[train_trans]\n",
    "        nnc_validate_tpz_features = nnr_test_tpz_features[val_trans]\n",
    "        \n",
    "        header = 'specz size g r i z y gr ri iz zy gri riz izy zphot zconf zerr eg er ei ez ey egr eri eiz ezy egri eriz eizy'\n",
    "        fmt = ' %.10f' + ' %.5f'*6 + ' %.5e' * 7 + ' %.5e'*12 + ' %.5e'*3 \n",
    "\n",
    "        # Save the NNC training and validation files\n",
    "\n",
    "        np.savetxt(directory + filename + '.nnc_train', np.hstack((nnc_train_specz.reshape(-1,1), nnc_train_features, nnc_train_tpz_features, nnc_train_feature_errs)), comments = '', header = header, fmt = fmt)\n",
    "        np.savetxt(directory + filename + '.nnc_validate', np.hstack((nnc_validate_specz.reshape(-1,1), nnc_validate_features, nnc_validate_tpz_features, nnc_validate_feature_errs)), comments = '', header = header, fmt = fmt)\n",
    "        # np.savetxt(directory + filename + '.nnc_test', np.hstack((nnc_test_specz.reshape(-1,1), nnc_test_features, nnc_test_tpz_features, nnc_test_feature_errs, nnc_test_features_lo, nnc_test_features_hi, nnc_test_features_frac)), comments = '', header = header, fmt = fmt)\n",
    "\n",
    "        print('finished: generate NNC files')\n",
    "        generate_input_files(directory, filename, inputdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, sample_frac, subrun = True)\n",
    "        #generate_input_files(directory, filename, inputdf, testingdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, train_frac_lim, test_frac_lim, subrun = True)\n",
    "\n",
    "    # ==================================\n",
    "    # GENERATE TPZ APPLICATION SET FILES\n",
    "    # ==================================\n",
    "\n",
    "    elif not os.path.isfile(directory + filename + '.tpz_app'):\n",
    "\n",
    "        print('Generating TPZ application set files...')\n",
    "\n",
    "        app_inds = np.loadtxt(directory + filename + '.app_inds', dtype = int)\n",
    "        app_specz, app_features, app_feature_errs = get_features(app_inds, inputdf, size=False)\n",
    "\n",
    "        header = 'specz g r i z y gr ri iz zy gri riz izy eg er ei ez ey egr eri eiz ezy egri eriz eizy'\n",
    "        fmt = ' %.10f' + ' %.5f'*5 + ' %.5e' * 7 + ' %.5e'*12\n",
    "\n",
    "        # Generate the TPZ application set\n",
    "\n",
    "        np.savetxt(directory + filename + '.tpz_app', np.hstack((app_specz.reshape(-1,1), app_features, app_feature_errs)), header = header, fmt = fmt)\n",
    "        print('finished: generate TPZ application set files')\n",
    "        generate_input_files(directory, filename, inputdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, sample_frac, subrun = True)\n",
    "        #generate_input_files(directory, filename, inputdf, testingdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, train_frac_lim, test_frac_lim, subrun = True)\n",
    "\n",
    "    # ==========================================\n",
    "   # GENERATE NNC AND NNR APPLICATION SET FILES\n",
    "   # ==========================================\n",
    "\n",
    "    elif not os.path.isfile(directory + filename + '.nn_app') and os.path.isfile(directory + 'output/results/' + filename + '.1.mlz'):\n",
    "\n",
    "        print('Generating NN application set files...')\n",
    "\n",
    "        app_inds = np.loadtxt(directory + filename + '.app_inds', dtype = int)\n",
    "        app_specz, app_features, app_feature_errs = get_features(app_inds, inputdf, size=True)\n",
    "\n",
    "        app_tpz_features = np.loadtxt(directory + 'output/results/' + filename + '.1.mlz', usecols = [2,4,6]) # 1,3,5 corresponds to zmode; 2,4,6 corresponds to zmean; is z, zconf, zerr\n",
    "       \n",
    "        header = 'specz size g r i z y gr ri iz zy gri riz izy zphot zconf zerr eg er ei ez ey egr eri eiz ezy egri eriz eizy '\n",
    "        fmt = ' %.10f' + ' %.5f'*6 + ' %.5e' * 7 + ' %.5e'*12 + ' %.5e'*3\n",
    "\n",
    "        # Generate the NN application set\n",
    "\n",
    "        np.savetxt(directory + filename + '.nn_app', np.hstack((app_specz.reshape(-1,1), app_features, app_tpz_features, app_feature_errs)), comments = '', header = header, fmt = fmt)\n",
    "\n",
    "        print('finished: generate NNC and NNR application set files')\n",
    "        generate_input_files(directory, filename, inputdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, sample_frac, subrun = True)\n",
    "        #generate_input_files(directory, filename, inputdf, testingdf, tpz_nnr_nnc_split, nn_train_val_split, overall_split, no_nnr, train_frac_lim, test_frac_lim, subrun = True)\n",
    "\n",
    "    elif not subrun:\n",
    "        print('All of these files already exist.  Please delete the input files before continuing.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_select_training(dataframe, unrep, sample):\n",
    "    '''\n",
    "    datafram is the pandas dataframe of all the galaxy photometry and true redshift information\n",
    "    unrep: True or False, do we want to select an unrepresentative training and application sample\n",
    "    sample: string, either 'DC2' or 'buzzard'\n",
    "    '''\n",
    "    \n",
    "    #dataframe = pd.DataFrame.from_dict(dataframe)\n",
    "\n",
    "    if unrep == True:\n",
    "        \n",
    "        #identify unique ratio values\n",
    "        unique_ratios = dataframe['ratios'].unique()\n",
    "        \n",
    "        #Select the indices of the dataframe to keep\n",
    "        keep_inds = []\n",
    "        for i in range(len(unique_ratios)):\n",
    "            temp_data = dataframe[dataframe['ratios'] == unique_ratios[i]]\n",
    "            number_to_keep = int(len(temp_data)*(1-unique_ratios[i]))\n",
    "            indices_to_list = temp_data.index.values.tolist()\n",
    "            for j in range(0, number_to_keep):\n",
    "                keep_inds.append(indices_to_list[j])\n",
    "        \n",
    "        \n",
    "        good_data = dataframe.loc[keep_inds,:]\n",
    "        index_list = good_data.index.values.tolist()\n",
    "    else:\n",
    "        index_list = dataframe.index.values.tolist()\n",
    "    \n",
    "    return index_list\n",
    "\n",
    "def data_select_testing(dataframe, unrep, sample):\n",
    "    '''\n",
    "    datafram is the pandas dataframe of all the galaxy photometry and true redshift information\n",
    "    unrep: True or False, do we want to select an unrepresentative training and application sample\n",
    "    sample: string, either 'DC2' or 'buzzard'\n",
    "    '''\n",
    "    #dataframe = pd.DataFrame.from_dict(dataframe)\n",
    "\n",
    "    if unrep == True:\n",
    "        \n",
    "        #identify unique ratio values\n",
    "        unique_ratios = dataframe['ratios'].unique()\n",
    "        \n",
    "        #Select the indices of the dataframe to keep\n",
    "        keep_inds = []\n",
    "        for i in range(len(unique_ratios)):\n",
    "            temp_data = dataframe[dataframe['ratios'] == unique_ratios[i]]\n",
    "            number_to_keep = int(len(temp_data)*unique_ratios[i])\n",
    "            indices_to_list = temp_data.index.values.tolist()\n",
    "            for j in range(0, number_to_keep):\n",
    "                keep_inds.append(indices_to_list[j])\n",
    "        \n",
    "        \n",
    "        good_data = dataframe.loc[keep_inds,:]\n",
    "        index_list = good_data.index.values.tolist()\n",
    "    else:\n",
    "        index_list = dataframe.index.values.tolist()\n",
    "    \n",
    "    return index_list\n",
    "    \n",
    "\n",
    "def default_test_train_split(indices, splitfrac):\n",
    "\n",
    "    split_len = len(indices)*splitfrac\n",
    "    split_len = int(split_len)\n",
    "\n",
    "    train_inds = indices[0:split_len]\n",
    "    test_inds = indices[split_len:]\n",
    "\n",
    "    return train_inds, test_inds\n",
    "\n",
    "def get_features(indices, dataframe, size):\n",
    "\n",
    "    #dataframe = pd.DataFrame.from_dict(dataframe)\n",
    "\n",
    "    speczs = dataframe['specz'].loc[indices]\n",
    "    speczs = speczs.to_numpy()\n",
    "\n",
    "    dataframe['gri'] = dataframe['gr']-dataframe['ri']\n",
    "    dataframe['egri'] = np.sqrt((dataframe['g_err']**2)+4*(dataframe['r_err']**2)+(dataframe['i_err']**2))\n",
    "    dataframe['riz'] = dataframe['ri']-dataframe['iz']\n",
    "    dataframe['eriz'] = np.sqrt((dataframe['r_err']**2)+4*(dataframe['i_err']**2)+(dataframe['z_err']**2))\n",
    "    dataframe['izy'] = dataframe['iz']-dataframe['zy']\n",
    "    dataframe['eizy'] = np.sqrt((dataframe['i_err']**2)+4*(dataframe['z_err']**2)+(dataframe['y_err']**2))\n",
    "\n",
    "    if size:\n",
    "        features = dataframe[['mcal_T', 'g', 'r', 'i', 'z', 'y', 'gr', 'ri', 'iz', 'zy', 'gri', 'riz', 'izy']].loc[indices]\n",
    "    if not size:\n",
    "        features = dataframe[['g', 'r', 'i', 'z', 'y', 'gr', 'ri', 'iz', 'zy', 'gri', 'riz', 'izy']].loc[indices]\n",
    "    features = features.to_numpy()\n",
    "    feature_errs = dataframe[['g_err', 'r_err', 'i_err', 'z_err', 'y_err', 'gr_err', 'ri_err', 'iz_err', 'zy_err', 'egri', 'eriz', 'eizy']].loc[indices]\n",
    "    feature_errs = feature_errs.to_numpy()\n",
    "\n",
    "    return speczs, features, feature_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e692c071-940a-4cf9-ac9b-c028e8c81c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating fresh NNR input files in nn_class/unrep/Buzzard/\n",
      "finished: generate NNR files\n",
      "Generating fresh NNC input files in nn_class/unrep/Buzzard/\n",
      "finished: generate NNC files\n",
      "Generating NN application set files...\n",
      "finished: generate NNC and NNR application set files\n"
     ]
    }
   ],
   "source": [
    "generate_input_files(directory=f'nn_class/unrep/Buzzard/', filename = 'tpzrun', inputdf = data, tpz_nnr_nnc_split = np.array([0.333, 0.666]), nn_train_val_split = 0.85, overall_split = 0.8, no_nnr = True, sample_frac = keep_frac, subrun = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28173c-0069-4be2-99e2-d10643a507f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ed094-8d2d-480f-ba62-13153138d8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python",
   "language": "python",
   "name": "desc-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
